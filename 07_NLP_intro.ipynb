{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9441af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a4b8b",
   "metadata": {},
   "source": [
    "문자열로 된 데이터 종류\n",
    "- 범주형 데이터 (ex. 색상, 성별 etc.)\n",
    "- 범주에 연결할 수 있는 임의의 문자열 (ex. 다홍색(비슷한 종류) >>> 빨강, 검점색(오타) >>> 검정)\n",
    "- 구조화된 문자열 데이터 (ex. 도로명주소)\n",
    "- 말 그대로 문자열 (ex. .txt 파일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558e374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호가 0000002F 0639:BF7F입니다.\n",
      "C:\\USERS\\RKSLA\\DOCUMENTS\\PYTHON\\BASIC\\ML\\PPB\\DATA\\ACLIMDB\n",
      "├─test\n",
      "│  ├─neg\n",
      "│  └─pos\n",
      "└─train\n",
      "    ├─neg\n",
      "    └─pos\n"
     ]
    }
   ],
   "source": [
    "!tree data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf385011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPEOF text_train:  <class 'list'>\n",
      "SIZEOF text_train:  25000\n",
      "text_train[6]: \n",
      "  b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"TYPEOF text_train: \", type(text_train))\n",
    "print(\"SIZEOF text_train: \", len(text_train))\n",
    "print(\"text_train[6]: \\n \", text_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad0a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\"\") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415f0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (TRAIN):  [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"클래스별 샘플 수 (TRAIN): \", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853ef4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수:  25000\n",
      "클래스별 샘플 수 (TEST): \n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"테스트 데이터의 문서 수: \", len(text_test))\n",
    "print(\"클래스별 샘플 수 (TEST): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5082cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = [doc.replace(b\"<br />\", b\"\") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9020e3",
   "metadata": {},
   "source": [
    "# BOW (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e349ce",
   "metadata": {},
   "source": [
    "과정\n",
    "1. 토큰화: 각 문서를 문서에 포함된 단어(토큰)로 나눈다. 이 때 기준은 띄어쓰기, 점 따위로 한다\n",
    "2. 어휘 사전 구축: 모든 문서에 나타난 모든 단어의 어휘를 모으고 번호를 매긴다. 순서는 알파벳 순으로 한다\n",
    "3. 인코딩: 어휘 사전의 단어가 문서마다 몇 번이나 나타나는지를 센다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c858a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words = [\n",
    "    \"the fool doth think he is wise, \", \n",
    "    \"but the wise man knows himself to be a fool\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce21e263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    feature_extraction.text.CountVectorizer: sklearn의 BOW 변환 도구\n",
    "    \n",
    "    상세 동작\n",
    "        1. vect.fit(문자열) 결과는 다음과 같다\n",
    "            1.1 전달한 문자열을 단어 경계 단위로 나눈다. 이 때 모든 단어는 소문자로 바꾼다\n",
    "            1.2 추출한 단어 목록의 unique 목록을 만들어 알파벳 순으로 정렬한다\n",
    "            \n",
    "        2. vect.transform(문자열) 의 결과는 다음과 같다\n",
    "            2.1 문자열을 희소 행렬로 바꾼다\n",
    "            2.2 ndarray일 때 값은 전달한 문자열에 해당 위치 단어가 몇 회 출현하는지에 대한 횟수다\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d27c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기:  13\n",
      "어휘 사전의 내용: \n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    vect.vocabulary_: BOW에 있는 단어 종류를 사전 형식으로 나타냄\n",
    "\"\"\"\n",
    "\n",
    "print(\"어휘 사전의 크기: \", len(vect.vocabulary_))\n",
    "print(\"어휘 사전의 내용: \\n\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "808d1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"BOW: \", repr(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW TO NDARRAY: \n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW TO NDARRAY: \\n\", bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f367bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      the\n",
       "1     fool\n",
       "2     doth\n",
       "3    think\n",
       "4       he\n",
       "5       is\n",
       "6    wise,\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds = pd.Series(bards_words[0].split(\" \")[ : -1])\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e960334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      the\n",
       "1     fool\n",
       "2     doth\n",
       "3    think\n",
       "4       he\n",
       "5       is\n",
       "6     wise\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds[6] = wrds[6][ : -1]\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43479da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doth', 'fool', 'he', 'is', 'the', 'think', 'wise']\n",
      "[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(wrds.sort_values().to_list())\n",
    "print(bag_of_words.toarray()[0])\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81f34c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = CountVectorizer()\n",
    "v2.fit([\"what is your major malfunction\"])\n",
    "\n",
    "v2.transform(['what you think is what you can be']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff04367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_wrds = [\"안녕하세요 저는 트위치에서 방송을 하고 있는 스트리머\"]\n",
    "\n",
    "kv = CountVectorizer()\n",
    "kv.fit(kor_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2955bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기:  7\n",
      "어휘 사전의 내용: \n",
      " {'안녕하세요': 2, '저는': 4, '트위치에서': 5, '방송을': 0, '하고': 6, '있는': 3, '스트리머': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: \", len(kv.vocabulary_))\n",
    "print(\"어휘 사전의 내용: \\n\", kv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "812cc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR_BOW:  <1x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "kr_bows = vect.transform(kor_wrds)\n",
    "print(\"KR_BOW: \", repr(kr_bows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a1c3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR_BOW TO NDARRAY: \n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"KR_BOW TO NDARRAY: \\n\", kr_bows.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2744a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      " <25000x75911 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431163 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train: \\n\", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b6dc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수:  75911\n",
      "특성 [0 : 20]:  ['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
      " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
      "특성 [20010 : 20030]:  ['doppelgangers' 'doppelgänger' 'dopplebangers' 'doppleganger' 'doppler'\n",
      " 'dopy' 'doqui' 'dor' 'dora' 'dorado' 'dorama' 'doran' 'dorcas' 'dorcey'\n",
      " 'dorday' 'dore' 'doreen' 'doremus' 'doren' 'dorf']\n",
      "특성 [ : : 2000]:  ['00' 'adultery' 'appearence' 'bang' 'blissfully' 'burress' 'chaulk'\n",
      " 'compensations' 'crossfire' 'derboiler' 'dop' 'empty' 'falling' 'formats'\n",
      " 'gisbourne' 'hallucinogenics' 'honore' 'inferenced' 'johnston' 'laconic'\n",
      " 'looping' 'matlin' 'modelling' 'nerdish' 'orion' 'periphery' 'preached'\n",
      " 'raha' 'resolving' 'salli' 'shaffer' 'smurfs' 'stereotypic'\n",
      " 'synchronized' 'tirard' 'una' 'venantini' 'whycome']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"특성 개수: \", len(feature_names))\n",
    "print(\"특성 [0 : 20]: \", feature_names[ : 20])\n",
    "print(\"특성 [20010 : 20030]: \", feature_names[20010 : 20030])\n",
    "print(\"특성 [ : : 2000]: \", feature_names[ : : 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfaf1e",
   "metadata": {},
   "source": [
    "#### 특성 추출 개선 전 rough data 그대로 썼을 때의 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afe28b",
   "metadata": {},
   "source": [
    "특성 추출 전에는 단수형/복수형을 서로 다른 단어로 인식하게되어 성능 저하가 발생할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33cb117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN(CROSS_VAL): 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, n_jobs=-1)\n",
    "print(\"MEAN(CROSS_VAL): {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5edc93d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST CROSS VAL SCORE: 0.89\n",
      "BEST PARAMS:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [10**x for x in range(-3, 2)]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))\n",
    "print(\"BEST PARAMS: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15187104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_SCORE: 0.88\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"TEST_SCORE: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "571deb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.32856240272522"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grid.cv_results_['mean_fit_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38c88",
   "metadata": {},
   "source": [
    "### 특성 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3335f",
   "metadata": {},
   "source": [
    "#### 1. min_df: 최소 n개 문서에서 나타난 단어만 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cf81388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df로 제한한 X_train:  <25000x27264 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3352876 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"min_df로 제한한 X_train: \", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9df0c048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE [ : 50] \n",
      " ['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
      " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
      " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
      " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
      " '16th']\n",
      "20010 ~ 20030 \n",
      " ['repetition' 'repetitions' 'repetitious' 'repetitive' 'rephrase'\n",
      " 'replace' 'replaced' 'replacement' 'replaces' 'replacing' 'replay'\n",
      " 'replayed' 'replaying' 'replays' 'replete' 'replica' 'replicas'\n",
      " 'replicate' 'replied' 'replies']\n",
      "[ : : 700] \n",
      " ['00' 'affectionate' 'appropriate' 'barbet' 'blur' 'butch' 'cheery'\n",
      " 'commit' 'courtroom' 'deconstruct' 'disgraceful' 'dvr' 'escort' 'fella'\n",
      " 'freezing' 'gorillas' 'havana' 'hunk' 'insist' 'juice' 'left' 'mafia'\n",
      " 'met' 'musicals' 'occult' 'parliament' 'pleasence' 'prop' 'recites'\n",
      " 'revisionist' 'sassy' 'shedding' 'sniff' 'stems' 'swear' 'thugs' 'tween'\n",
      " 'vanish' 'whale']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"FEATURE [ : 50] \\n\", feature_names[ : 50])\n",
    "print(\"20010 ~ 20030 \\n\", feature_names[20010 : 20030])\n",
    "print(\"[ : : 700] \\n\", feature_names[ : : 700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6730fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST CROSS VAL SCORE: 0.89\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea96d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.512445545196535"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(grid.cv_results_['mean_fit_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c008b",
   "metadata": {},
   "source": [
    "최소 출현 문서 수로 규제했을 때의 효과\n",
    "1. 성능이 좋아진 것은 아니다\n",
    "2. 대신 단어장 크기가 줄어 훈련 시간이 줄었다 (평균 38초 >>> 23초로 감소)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d82b72",
   "metadata": {},
   "source": [
    "#### 2. 불용어 제외: 불필요한 관사, 조사, 전치사, 접속사 등은 사전에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"SIZEOF STOPWORDS: \", len(ENGLISH_STOP_WORDS))\n",
    "print(list(ENGLISH_STOP_WORDS)[ : : 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_word=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisiticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cad1d",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "param_grid = {'logisticregression__C': [10**x for x in range(-3, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39654479",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"LOW-tfidf FEATURES: \\n\", feature_naems[sorted_by_tfidf[ : 20]])\n",
    "print(\"HIGH-tfidf FEATURES: \\n\", feature_naems[sorted_by_tfidf[-20 : ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c748ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"LOWEST-idf FEATURES: \\n\", feature_names[sorted_by_idf[ : 100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps['logisticregression'].coef_[0],\n",
    "    feature_names, n_top_features=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bards_words: \\n', bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb48e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b422c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [10**x for x in range(-3, 3)],\n",
    "    'tfidfVectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"BEST CV SCORE: {:.2f}\".format(grid.best_score_))\n",
    "print(\"BEST PARAMS: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "\n",
    "heatmap = mglearn.tools.heatmap(\n",
    "    scores,\n",
    "    xlabel='C', ylabel='ngram_range',\n",
    "    xticklabels=param_grid['logisticregression__C'], yticklabels=param_grid['tfidfvectorizer__ngram_range']\n",
    "    cmap='viridis',\n",
    "    fmt=\"%.3f\",\n",
    ")\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_naes = np.array(vect.get_feature_names_out())\n",
    "coef = grid.best_estimator_.naemd_steps['logistigregression'].coef_\n",
    "mgelarn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e846e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.__version__\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp=  spacy.load('en_core_web_sm')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def compare_normalization(doc):\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    print(\"표제어: \")\n",
    "    print([token.lenna_ for token in doc_spacy])\n",
    "    print(\"어간: \")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(\n",
    "    u\"Our meeting today was worse than yesterday, \"\n",
    "    \"I'm scared of meeting the clients tomorrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma for token in doc_spacy]\n",
    "\n",
    "lemma_vect = CountVectoricer(tokenizer=custom_tokenizer, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "print(\"X_train_lemma.shape: \", X_train_lemma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf127ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train.shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "param_grid = {'C': [10**x for x in range(-3, 3)]}\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99, train_size=0.01, random_state=0)\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CV SCORE(countervectorizer): \", grid.best_score_)\n",
    "\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "print(\"BEST CV SCORE(lemma): \", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=0.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method='batch', max_iter=25, random_state=0, n_jobs=-1)\n",
    "\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce53012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lda.components_.shape\", lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d90cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components, axis=1)[ : , : : -1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(10),\n",
    "    feature_names=feature_names, \n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda100 = LatentDirichletAllocation(\n",
    "    n_components=100,\n",
    "    learning_method='batch',\n",
    "    max_iter=25,\n",
    "    random_state=0, n_jobs=-1\n",
    ")\n",
    "\n",
    "document_topics100 = lda100.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array([7, 16, 24, 25, 28, 36, 37, 41, 45, 51, 53, 54, 63, 89, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda100.components_, axis=1)[ : , : : -1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "mglearn.tools.print_topics(\n",
    "    topics=topics, \n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting, topics_per_chunk=5, n_words=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "music = np.argsort(document_topics100[ : , 45])[ : : -1]\n",
    "\n",
    "for i in music[ : 10] :\n",
    "    print(b\".\".join(text_train[i].split(b\".\")[ : 2]) + b\".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "topic_names = [\"{:>2} \".format(i) + \" \".join(words) for i, words in enumerate(feature_names[sorting[ : , : 2]])]\n",
    "\n",
    "forcol in [0, 1] :\n",
    "    start = col * 50\n",
    "    end = (col + ) * 50\n",
    "    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start : end])\n",
    "    ax[col].set_yticks(np.arange(50))\n",
    "    ax[col].set_yticklabels(topic_names[start : end], ha='left', va='top')\n",
    "    ax[col].invert_yaxis()\n",
    "    ax[col].set_xlim(0, 2000)\n",
    "    yax = ax[col].get_yaxis()\n",
    "    yax.set_tick_params(pad=130)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab4e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
