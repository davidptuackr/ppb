{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9441af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558e374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호가 000000C1 0639:BF7F입니다.\n",
      "C:\\USERS\\RKSLA\\DOCUMENTS\\PYTHON\\BASIC\\ML\\PPB\\DATA\\ACLIMDB\n",
      "├─test\n",
      "│  ├─neg\n",
      "│  └─pos\n",
      "└─train\n",
      "    ├─neg\n",
      "    ├─pos\n",
      "    └─unsup\n"
     ]
    }
   ],
   "source": [
    "!tree data/aclImdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf385011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPEOF text_train:  <class 'list'>\n",
      "SIZEOF text_train:  75000\n",
      "text_train[6]: \n",
      "  b'Gloomy Sunday - Ein Lied von Liebe und Tod directed by Rolf Sch\\xc3\\xbcbel in 1999 is a romantic, absorbing, beautiful, and heartbreaking movie. It started like Jules and Jim; it ended as one of Agatha Christie\\'s books, and in between it said something about love, friendship, devotion, jealousy, war, Holocaust, dignity, and betrayal, and it did better than The Black Book which is much more popular. It is not perfect, and it made me, a cynic, wonder in the end on the complexity of the relationships and sensational revelations, and who is who to whom but the movie simply overwhelmed me. Perfect or not, it is unforgettable. All four actors as the parts of the tragic not even a triangle but a rectangle were terrific. I do believe that three men could fell deeply for one girl as beautiful and dignified as Ilona in a star-making performance by young Hungarian actress Erica Marozs\\xc3\\xa1n and who would not? The titular song is haunting, sad, and beautiful, and no doubt deserves the movie been made about it and its effect on the countless listeners. I love the movie and I am surprised that it is so little known in this country. It is a gem.<br /><br />The fact that it is based on a story of the song that had played such important role in the lives of all characters made me do some research, and the real story behind the song of Love and Death seems as fascinating as the fictional one. The song was composed in 1930s by Rezs\\xc3\\xb6 Seress and was believed to have caused many suicides in Hungary and all over Europe as the world was moving toward the most devastating War of the last century. Rezs\\xc3\\xb6 Seress, a Jewish-Hungarian pianist and composer, was thrown to the Concentration Camp but survived, unlike his mother. In January, 1968, Seress committed suicide in Budapest by jumping out of a window. According to his obituary in the New York Times, \"Mr. Seres complained that the success of \"Gloomy Sunday\" actually increased his unhappiness, because he knew he would never be able to write a second hit.\" <br /><br />Many singers from all over the world have recorded their versions of the songs in different languages. Over 70 performers have covered the song since 1935, and some famous names include Billie Holiday, Paul Robeson, Pyotr Leschenko (in Russian, under title \"Mratschnoje Woskresenje\"), Bjork, Sarah McLachlan, and many more. The one that really got to me and made me shiver is by Diamanda Gal\\xc3\\xa1s, the Greek born American singer/pianist/performer with the voice of such tragic power that I still can\\'t get over her singing. Gal\\xc3\\xa1s has been described as \"capable of the most unnerving vocal terror\", and in her work she mostly concentrates on the topics of \"suffering, despair, condemnation, injustice and loss of dignity.\" When she sings the Song of Love and Death, her voice that could\\'ve belonged to the most tragic heroines of Ancient Greece leaves no hope and brings the horror and grief of love lost forever to the unbearable and incomparable heights.<br /><br />8.5/10'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"TYPEOF text_train: \", type(text_train))\n",
    "print(\"SIZEOF text_train: \", len(text_train))\n",
    "print(\"text_train[6]: \\n \", text_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad0a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\"\") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415f0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (TRAIN):  [12500 12500 50000]\n"
     ]
    }
   ],
   "source": [
    "print(\"클래스별 샘플 수 (TRAIN): \", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853ef4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수:  25000\n",
      "클래스별 샘플 수 (TEST): \n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"테스트 데이터의 문서 수: \", len(text_test))\n",
    "print(\"클래스별 샘플 수 (TEST): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5082cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = [doc.replace(b\"<br />\", b\"\") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c858a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words = [\n",
    "    \"the fool doth think he is wise, \", \n",
    "    \"but the wise man knows himself to be a fool\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce21e263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d27c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기:  13\n",
      "어휘 사전의 내용: \n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: \", len(vect.vocabulary_))\n",
    "print(\"어휘 사전의 내용: \\n\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "808d1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW:  <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"BOW: \", repr(bag_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa6ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW TO NDARRAY: \n",
      " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW TO NDARRAY: \\n\", bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f367bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      the\n",
       "1     fool\n",
       "2     doth\n",
       "3    think\n",
       "4       he\n",
       "5       is\n",
       "6    wise,\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds = pd.Series(bards_words[0].split(\" \")[ : -1])\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e960334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      the\n",
       "1     fool\n",
       "2     doth\n",
       "3    think\n",
       "4       he\n",
       "5       is\n",
       "6     wise\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds[6] = wrds[6][ : -1]\n",
    "wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43479da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doth', 'fool', 'he', 'is', 'the', 'think', 'wise']\n",
      "[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      "{'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(wrds.sort_values().to_list())\n",
    "print(bag_of_words.toarray()[0])\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff04367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_wrds = [\"안녕하세요 저는 트위치에서 방송을 하고 있는 스트리머\"]\n",
    "\n",
    "kv = CountVectorizer()\n",
    "kv.fit(kor_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef2955bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기:  7\n",
      "어휘 사전의 내용: \n",
      " {'안녕하세요': 2, '저는': 4, '트위치에서': 5, '방송을': 0, '하고': 6, '있는': 3, '스트리머': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: \", len(kv.vocabulary_))\n",
    "print(\"어휘 사전의 내용: \\n\", kv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "812cc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR_BOW:  <1x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 0 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "kr_bows = vect.transform(kor_wrds)\n",
    "print(\"KR_BOW: \", repr(kr_bows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a1c3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR_BOW TO NDARRAY: \n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"KR_BOW TO NDARRAY: \\n\", kr_bows.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2744a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      " <75000x127229 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 10315468 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train: \\n\", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b6dc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수:  127229\n",
      "특성 [0 : 20]:  ['00' '000' '0000' '0000000000000000000000000000000001' '0000000000001'\n",
      " '000000001' '000000003' '00000001' '000001745' '00001' '0001' '00015'\n",
      " '0002' '0007' '00083' '000ft' '000s' '000th' '001' '002']\n",
      "특성 [20010 : 20030]:  ['cetait' 'cetera' 'cetin' 'cetmes' 'cetnik' 'cetniks' 'ceusescu' 'ceuta'\n",
      " 'ceylon' 'ceylonese' 'ceylons' 'cezanne' 'cezary' 'cezmi' 'cf' 'cfdc'\n",
      " 'cfm' 'cfto' 'cg' 'cga']\n",
      "특성 [ : : 2000]:  ['00' '9out' 'ages' 'andress' 'aryeman' 'baio' 'bellah' 'bloodstain'\n",
      " 'briers' 'calm' 'cessation' 'circenses' 'complementing' 'countedsix'\n",
      " 'dahan' 'dench' 'discourages' 'dreamkeeeper' 'elderly' 'esposito' 'fang'\n",
      " 'flaquer' 'frizzyhead' 'gerry' 'grandness' 'halycon' 'hesitated'\n",
      " 'hubiriffic' 'incongruence' 'ireperable' 'journal' 'kindsa' 'landingham'\n",
      " 'limply' 'maaaybbbeee' 'marthe' 'mephestophelion' 'modem' 'mushing'\n",
      " 'nigger' 'oghris' 'oxbridge' 'pensamentos' 'pleaaaaaaaase' 'prettified'\n",
      " 'quantify' 'recommendation' 'retrieving' 'rp' 'scam' 'sequiter' 'sidemen'\n",
      " 'snk' 'sprite' 'stroptomycin' 'swings' 'teoe' 'toiled' 'tsiolkovsky'\n",
      " 'unflagging' 'vaporised' 'wakens' 'wilshire' 'ynis']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"특성 개수: \", len(feature_names))\n",
    "print(\"특성 [0 : 20]: \", feature_names[ : 20])\n",
    "print(\"특성 [20010 : 20030]: \", feature_names[20010 : 20030])\n",
    "print(\"특성 [ : : 2000]: \", feature_names[ : : 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfaf1e",
   "metadata": {},
   "source": [
    "#### 특성 추출 개선 전 rough data 그대로 썼을 때의 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, n_jobs=-1)\n",
    "print(\"MEAN(CROSS_VAL): {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [10**x for x in range(-3, 2)]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))\n",
    "print(\"BEST PARAMS: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15187104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"TEST_SCORE: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38c88",
   "metadata": {},
   "source": [
    "#### 특성 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf81388",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"min_df로 제한한 X_train: \", repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"FEATURE [ : 50] \\n\", feature_names[ : 50])\n",
    "print(\"20010 ~ 20030 \\n\", feature_names[20010 : 20030])\n",
    "print(\"[ : : 700] \\n\", feature_names[ : : 700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d82b72",
   "metadata": {},
   "source": [
    "# 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"SIZEOF STOPWORDS: \", len(ENGLISH_STOP_WORDS))\n",
    "print(list(ENGLISH_STOP_WORDS)[ : : 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_word=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(repr(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(LogisiticRegression(max_iter=5000), param_grid, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cad1d",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "param_grid = {'logisticregression__C': [10**x for x in range(-3, 3)]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "grid.fit(text_train, y_train)\n",
    "print(\"BEST CROSS VAL SCORE: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39654479",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "\n",
    "X_train = vectorizer.transform(text_train)\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"LOW-tfidf FEATURES: \\n\", feature_naems[sorted_by_tfidf[ : 20]])\n",
    "print(\"HIGH-tfidf FEATURES: \\n\", feature_naems[sorted_by_tfidf[-20 : ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c748ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"LOWEST-idf FEATURES: \\n\", feature_names[sorted_by_idf[ : 100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps['logisticregression'].coef_[0],\n",
    "    feature_names, n_top_features=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bards_words: \\n', bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(1, 1)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb48e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(2, 2)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b422c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CounterVectrizer(ngram_range=(1, 3)).fit(bards_words)\n",
    "print(\"어휘 사전 크기: \", len(cv.vocabulary_))\n",
    "print(cv.get_features_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=5000))\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [10**x for x in range(-3, 3)],\n",
    "    'tfidfVectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"BEST CV SCORE: {:.2f}\".format(grid.best_score_))\n",
    "print(\"BEST PARAMS: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadbcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n",
    "\n",
    "heatmap = mglearn.tools.heatmap(\n",
    "    scores,\n",
    "    xlabel='C', ylabel='ngram_range',\n",
    "    xticklabels=param_grid['logisticregression__C'], yticklabels=param_grid['tfidfvectorizer__ngram_range']\n",
    "    cmap='viridis',\n",
    "    fmt=\"%.3f\",\n",
    ")\n",
    "\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "feature_naes = np.array(vect.get_feature_names_out())\n",
    "coef = grid.best_estimator_.naemd_steps['logistigregression'].coef_\n",
    "mgelarn.tools.visualize_coefficients(coef[0], feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n",
    "\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e846e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "\n",
    "nltk.__version__\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp=  spacy.load('en_core_web_sm')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def compare_normalization(doc):\n",
    "    doc_spacy = en_nlp(doc)\n",
    "    print(\"표제어: \")\n",
    "    print([token.lenna_ for token in doc_spacy])\n",
    "    print(\"어간: \")\n",
    "    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization(\n",
    "    u\"Our meeting today was worse than yesterday, \"\n",
    "    \"I'm scared of meeting the clients tomorrow\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
